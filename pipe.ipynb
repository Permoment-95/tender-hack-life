{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T18:20:20.722065Z",
     "iopub.status.busy": "2021-06-12T18:20:20.721774Z",
     "iopub.status.idle": "2021-06-12T18:20:21.456169Z",
     "shell.execute_reply": "2021-06-12T18:20:21.455317Z",
     "shell.execute_reply.started": "2021-06-12T18:20:20.722032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-13 14:52:34--  https://www.artlebedev.ru/country-list/tab/\n",
      "Resolving www.artlebedev.ru (www.artlebedev.ru)... 195.218.200.21\n",
      "Connecting to www.artlebedev.ru (www.artlebedev.ru)|195.218.200.21|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30505 (30K) [text/plain]\n",
      "Saving to: ‘country.csv’\n",
      "\n",
      "country.csv         100%[===================>]  29.79K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2021-06-13 14:52:34 (694 KB/s) - ‘country.csv’ saved [30505/30505]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O country.csv https://www.artlebedev.ru/country-list/tab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[38;5;1m✘ No compatible model found for 'u_core_news_md' (spaCy v2.3.0).\u001b[0m\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download u_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T20:13:13.162487Z",
     "iopub.status.busy": "2021-06-12T20:13:13.162328Z",
     "iopub.status.idle": "2021-06-12T20:13:13.182863Z",
     "shell.execute_reply": "2021-06-12T20:13:13.182027Z",
     "shell.execute_reply.started": "2021-06-12T20:13:13.162473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/storage1/ryazantsev/AminoEmbeddings/TenderHack/SkuChangeRequests.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T05:21:07.850258Z",
     "iopub.status.busy": "2021-06-12T05:21:07.850020Z",
     "iopub.status.idle": "2021-06-12T05:21:07.857055Z",
     "shell.execute_reply": "2021-06-12T05:21:07.855653Z",
     "shell.execute_reply.started": "2021-06-12T05:21:07.850231Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_ru = ['Номер заявки', 'Форма заявки', 'Наименование', 'Единица измерения', 'Изображение', 'Классификация ГОСТ/ТУ', 'Описание', 'Статус', 'Причина отказа', 'Комментарий', 'Ид оферты', 'Наименование оферты', 'Артикул оферты', 'Регион поставки', 'Срок поставки в днях От', 'Срок поставки в днях До', 'Доступное количество От', 'Доступное количество До', 'Исходные характеристики', 'Категория оферты', 'Категория справочника', 'Вид продукции', 'Количество эталонных утвержденных характеристик в категории', 'Количество использованных поставщиком эталонных утвержденных характеристик']\n",
    "columns_en = ['application_num', 'application_form', 'name', 'measurement_unit', 'image', 'gost_calss', 'description', 'status', 'reason', 'comment' , 'offer_id', 'offer_name', 'offer_number', 'delivery_region', 'delivery_from','delivery_to', 'quantity_from', 'quantity_to', 'init_char', 'offer_category', 'directory_category', 'product_type', 'number_characteristics', 'number_characteristics_supplier']\n",
    "columns_translator = {ru: eng for ru, eng in zip(columns_ru, columns_en)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:03:18.082317Z",
     "iopub.status.busy": "2021-06-12T10:03:18.082153Z",
     "iopub.status.idle": "2021-06-12T10:03:23.316607Z",
     "shell.execute_reply": "2021-06-12T10:03:23.315991Z",
     "shell.execute_reply.started": "2021-06-12T10:03:18.082302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Перевод колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:03:24.058216Z",
     "iopub.status.busy": "2021-06-12T10:03:24.058017Z",
     "iopub.status.idle": "2021-06-12T10:03:24.107347Z",
     "shell.execute_reply": "2021-06-12T10:03:24.106135Z",
     "shell.execute_reply.started": "2021-06-12T10:03:24.058199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns=columns_translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формирование датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:03:24.108887Z",
     "iopub.status.busy": "2021-06-12T10:03:24.108559Z",
     "iopub.status.idle": "2021-06-12T10:03:24.117897Z",
     "shell.execute_reply": "2021-06-12T10:03:24.117226Z",
     "shell.execute_reply.started": "2021-06-12T10:03:24.108810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "REASON_NUM = 55\n",
    "\n",
    "\n",
    "def sep_reason(reason: str):\n",
    "    if pd.notna(reason):\n",
    "        return reason.split(';')\n",
    "    return reason\n",
    "\n",
    "def get_reason_freq(df: pd.DataFrame):\n",
    "    reasons_df = df.reason.dropna().apply(sep_reason)\n",
    "    reas = [item for sublist in reasons_df.to_list() for item in sublist]\n",
    "    reas = pd.DataFrame(reas, columns=['reason'])\n",
    "    return reas.reason.value_counts()\n",
    "\n",
    "def get_reason_dict(df: pd.DataFrame):\n",
    "    reasons_df = df.reason.dropna().apply(sep_reason)\n",
    "    reas = [item for sublist in reasons_df.to_list() for item in sublist]\n",
    "    reas = pd.DataFrame(reas, columns=['reason'])\n",
    "    reject_reasons = {rej: i for i, rej in  enumerate(reas.reason.value_counts().index.to_list())}\n",
    "    return reject_reasons\n",
    "\n",
    "def get_reject_reason_vector_index(reason, reject_reasons):\n",
    "    if isinstance(reason, list):\n",
    "        return [reject_reasons.get(reas, None) for reas in reason]\n",
    "    return []\n",
    "    \n",
    "def get_reject_reason_vector(reason_index):\n",
    "    vec = np.zeros(REASON_NUM, dtype=int)\n",
    "    vec[reason_index] = 1\n",
    "    return vec\n",
    "\n",
    "def get_reject_comment_vector(reason_index):\n",
    "    vec = np.zeros(REASON_NUM, dtype=int)\n",
    "    vec[reason_index] = 1\n",
    "    return vec\n",
    "\n",
    "def reject_index(reject_vec):\n",
    "    return pd.Series(data=reject_vec, index=[f'reason_{i}' for i in range(len(reject_vec))])\n",
    "\n",
    "def get_right_df(df: pd.DataFrame):\n",
    "    df['status_bit'] = df.status.apply(lambda x: 0 if x == 'Принята' else 1)\n",
    "    df.loc[((df.status_bit == 0) & (df.comment.notna())), 'comment'] = np.nan\n",
    "    df.loc[((df.status_bit == 0) & (df.reason.notna())), 'reason'] = np.nan\n",
    "    reject_reasons = get_reason_dict(df)\n",
    "    df['reason_list'] = df.reason.apply(sep_reason)\n",
    "    df['reason_index'] = df['reason_list'].apply(lambda x: get_reject_reason_vector_index(x, reject_reasons))\n",
    "    df['reason_vec'] = df['reason_index'].apply(get_reject_reason_vector)\n",
    "    df['reason_count'] = df['reason_vec'].apply(sum)\n",
    "    reason_indx_df = df.reason_vec.apply(reject_index)\n",
    "    df = pd.concat([df, reason_indx_df], axis=1)\n",
    "    df['offer_subcategory'] = df.offer_category.apply(lambda x: x.split('-')[1])\n",
    "    df = df.drop(columns=['product_type'])\n",
    "    df = df.drop(index=df[(df.status_bit == 1) & (df.comment.isna()) & (df.reason.isna())].index)\n",
    "    df = df.dropna(subset=['name'])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T10:03:24.118970Z",
     "iopub.status.busy": "2021-06-12T10:03:24.118812Z",
     "iopub.status.idle": "2021-06-12T10:04:07.529469Z",
     "shell.execute_reply": "2021-06-12T10:04:07.528348Z",
     "shell.execute_reply.started": "2021-06-12T10:03:24.118953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = get_right_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T16:19:24.030589Z",
     "iopub.status.busy": "2021-06-12T16:19:24.030302Z",
     "iopub.status.idle": "2021-06-12T16:19:27.542252Z",
     "shell.execute_reply": "2021-06-12T16:19:27.541642Z",
     "shell.execute_reply.started": "2021-06-12T16:19:24.030559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_init_char_names(df):\n",
    "    def get_names(init_char):\n",
    "        return [item['name'].lower().capitalize() if item['name'] else item['name'] for item in json.loads(init_char)]\n",
    "\n",
    "    names = df.init_char.apply(get_names)\n",
    "    return names\n",
    "\n",
    "def parse_init_char_values(df):\n",
    "    def get_value(init_char):\n",
    "        return [item['value'] for item in json.loads(init_char)]\n",
    "\n",
    "    values = df.init_char.apply(get_value)\n",
    "    return values\n",
    "\n",
    "def parse_init_char_units(df):\n",
    "    def get_unit(init_char):\n",
    "        items = json.loads(init_char)\n",
    "        if items:    \n",
    "            if items[0]['name'] == \"Производитель\":\n",
    "                items = items[1:]\n",
    "        return [item.get('unitCode', None) for item in items]\n",
    "\n",
    "    units = df.init_char.apply(get_unit)\n",
    "    return units\n",
    "\n",
    "#### Имена характеристик по частоте встречаемости\n",
    "\n",
    "char_names = parse_init_char_names(df)\n",
    "char_names = [item for sublist in char_names.to_list() for item in sublist]\n",
    "\n",
    "char_df = pd.DataFrame(char_names, columns=['char']).dropna()\n",
    "\n",
    "char_df = char_df.char.apply(lambda x: x).value_counts().to_frame().reset_index()\n",
    "\n",
    "#### Значения характеристик\n",
    "\n",
    "char_values = parse_init_char_values(df)\n",
    "# char_values = [item for sublist in char_values.to_list() for item in sublist]\n",
    "\n",
    "# char_values_df = pd.DataFrame(char_values, columns=['char']).dropna()\n",
    "\n",
    "# char_values_df = char_values_df.char.apply(lambda x: x.lower()).value_counts().to_frame().reset_index()\n",
    "\n",
    "#### Ед.из. характеристик по частоте встречаемости\n",
    "\n",
    "char_units = parse_init_char_units(df)\n",
    "char_units = [item for sublist in char_units.to_list() for item in sublist]\n",
    "\n",
    "char_units_df = pd.DataFrame(char_units, columns=['char']).dropna()\n",
    "\n",
    "char_units_df = char_units_df.char.apply(lambda x: x.lower()).value_counts().to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T16:28:46.919111Z",
     "iopub.status.busy": "2021-06-12T16:28:46.918884Z",
     "iopub.status.idle": "2021-06-12T16:28:55.672843Z",
     "shell.execute_reply": "2021-06-12T16:28:55.671935Z",
     "shell.execute_reply.started": "2021-06-12T16:28:46.919093Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['char_names'] = parse_init_char_names(df)\n",
    "df['char_values'] = parse_init_char_values(df)\n",
    "df['char_units'] = parse_init_char_units(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг описания и попытка вытащить от туда характеристики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T13:36:33.432374Z",
     "iopub.status.busy": "2021-06-12T13:36:33.432075Z",
     "iopub.status.idle": "2021-06-12T13:36:33.437352Z",
     "shell.execute_reply": "2021-06-12T13:36:33.436432Z",
     "shell.execute_reply.started": "2021-06-12T13:36:33.432330Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def description_parser(description):\n",
    "    found_chars = []\n",
    "    if pd.notna(description):\n",
    "        chars_names = re.findall(r'([А-Я][а-я\\s]+)\\s*[:|-|—]\\s*', description)\n",
    "        chars_val = re.split(r'[А-Я][а-я\\s]+\\s*[:|-|—]\\s*', description)\n",
    "        if len(chars_val) > len(char_names):\n",
    "            chars_val = chars_val[1:]\n",
    "        for char, val in zip(chars_names, chars_val):\n",
    "            if char.lower() in char_df['index'].to_list():\n",
    "                found_chars.append({'name': char.lower().capitalize(), 'value': val})\n",
    "        if found_chars:\n",
    "            return found_chars\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T18:14:44.866503Z",
     "iopub.status.busy": "2021-06-12T18:14:44.866174Z",
     "iopub.status.idle": "2021-06-12T18:16:50.707549Z",
     "shell.execute_reply": "2021-06-12T18:16:50.706285Z",
     "shell.execute_reply.started": "2021-06-12T18:14:44.866464Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-682589cf1d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'descr_char'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription_parser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-95b6e87e79f2>\u001b[0m in \u001b[0;36mdescription_parser\u001b[0;34m(description)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mchars_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([А-Я][а-я\\s]+)\\s*[:|-|—]\\s*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mchars_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[А-Я][а-я\\s]+\\s*[:|-|—]\\s*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mchars_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchars_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'char_names' is not defined"
     ]
    }
   ],
   "source": [
    "df['descr_char'] = df.description.apply(description_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T23:38:00.482168Z",
     "iopub.status.busy": "2021-06-12T23:38:00.481843Z",
     "iopub.status.idle": "2021-06-12T23:38:00.485698Z",
     "shell.execute_reply": "2021-06-12T23:38:00.485045Z",
     "shell.execute_reply.started": "2021-06-12T23:38:00.482132Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:18:44.618807Z",
     "iopub.status.busy": "2021-06-13T06:18:44.618605Z",
     "iopub.status.idle": "2021-06-13T06:18:44.624825Z",
     "shell.execute_reply": "2021-06-13T06:18:44.623982Z",
     "shell.execute_reply.started": "2021-06-13T06:18:44.618761Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseStep:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = 'BaseStep',\n",
    "        columns: List = [],\n",
    "        pos_status: str = '',\n",
    "        neg_status: str = '',\n",
    "    ):\n",
    "        self.__pre_init__()\n",
    "        self.name = name\n",
    "        self.columns = columns\n",
    "        self.pos_status = pos_status\n",
    "        self.neg_status = neg_status\n",
    "        self.__post_init__()\n",
    "        \n",
    "    def __pre_init__(self,):\n",
    "        pass\n",
    "    \n",
    "    def __post_init__(self,):\n",
    "        pass\n",
    "    \n",
    "    def _eval_function(self, args):\n",
    "        return args\n",
    "    \n",
    "    def evaluate(self, df: pd.DataFrame, return_status: bool = False):\n",
    "        df_eval = df[self.columns if len(self.columns) > 1 else self.columns[0]]\n",
    "        if isinstance(df_eval, pd.Series):\n",
    "            results = df_eval.apply(self._eval_function)\n",
    "        else:\n",
    "            results = df_eval.apply(self._eval_function, axis=int(len(self.columns) > 1))\n",
    "        results.columns = [self.name]\n",
    "        results = results.to_frame()\n",
    "        if return_status:\n",
    "            status = pd.DataFrame([self.pos_status if res else self.neg_status for res in results.values], index=results.index, columns=[self.name + '_status'])\n",
    "            results = (results, status)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T06:36:52.643009Z",
     "iopub.status.busy": "2021-06-13T06:36:52.642764Z",
     "iopub.status.idle": "2021-06-13T06:36:52.651432Z",
     "shell.execute_reply": "2021-06-13T06:36:52.650745Z",
     "shell.execute_reply.started": "2021-06-13T06:36:52.642981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str = 'Pipeline',\n",
    "        steps = [],\n",
    "        pos_status: str = 'Отказать',\n",
    "        neg_status: str = 'Принять',\n",
    "    ):\n",
    "        self.__pre_init__()\n",
    "        self.name = name\n",
    "        self.steps = steps\n",
    "        self.pos_status = pos_status\n",
    "        self.neg_status = neg_status\n",
    "        self.__post_init__()\n",
    "        \n",
    "    def __pre_init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def __post_init__(self, ):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, df: pd.DataFrame, return_status: bool = False):\n",
    "        results = []\n",
    "        results_status = []\n",
    "        for step in self.steps:\n",
    "            if return_status:\n",
    "                res, status = step.evaluate(df, return_status=return_status)\n",
    "                results.append(res)\n",
    "                results_status.append(status)\n",
    "            else:\n",
    "                res = step.evaluate(df, return_status=return_status)\n",
    "                results.append(res)\n",
    "        results = pd.DataFrame(pd.concat(results, axis=1).any(axis=1), columns=[self.name], index=df.index)\n",
    "        \n",
    "        if return_status:\n",
    "            results_status = pd.concat(results_status, axis=1)\n",
    "            status = pd.DataFrame([self.pos_status if res else self.neg_status for res in results.values], index=results.index, columns=[self.name + '_status'])\n",
    "            results = (results, status, results_status)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from pyaspeller import YandexSpeller\n",
    "from transliterate import translit\n",
    "import regex as re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "import spacy\n",
    "from spacy.lang.ru.examples import sentences\n",
    "import json\n",
    "\n",
    "#nlp = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "# data = pd.read_csv('table_data.csv', sep=',')\n",
    "#\n",
    "with open('obscene_corpus.txt', 'r') as f:\n",
    "    file = f.readlines()\n",
    "\n",
    "with open('forbidden_list.txt', 'r') as f:\n",
    "    fb_file = f.readlines()\n",
    "\n",
    "\n",
    "#preprocessing of vocabulary of obscene words\n",
    "def vocab_lower(vocabulary: list):\n",
    "    list_of_lower_words = []\n",
    "    for word in vocabulary:\n",
    "        list_of_lower_words.append(word.lower().replace('\\n',''))\n",
    "    return list_of_lower_words\n",
    "\n",
    "vocab_obscene = vocab_lower(file)\n",
    "vocab_forbidden = vocab_lower(fb_file)\n",
    "\n",
    "# Токенизация текста\n",
    "def tokenize_text(text: str):\n",
    "    tokens_list = word_tokenize(text)\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "# Приведение текста к нижнему регистру\n",
    "def lower_text(text: str):\n",
    "    return (text.lower())\n",
    "\n",
    "\n",
    "# Транслитерация текста (латиница -> кириллица)\n",
    "def translit_chars(text: str):\n",
    "    return (translit(text, 'ru', reversed=False))\n",
    "\n",
    "\n",
    "# Удаление знаков препинания\n",
    "def re_gex(text: str):\n",
    "    nabor = re.compile(r'[.+-,!@\"*#$%^&)(|\\/?=_:;]')\n",
    "    text_clean = nabor.sub(r' ', text)\n",
    "    return text_clean\n",
    "\n",
    "\n",
    "# Функция препроцессинга текста\n",
    "def text_preprocess(text: str):\n",
    "    '''Функция препроцессинга текста: токенизация,\n",
    "        приведение к нижнему регистру,\n",
    "        транслитерация и удаление знаков'''\n",
    "\n",
    "    tokens_list = tokenize_text(lower_text(translit_chars(re_gex(text))))\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "# Исправление опечаток\n",
    "def correct_typos(tokens_list: list):\n",
    "    '''Функция принимает на вход список токенов и заменяет грамматически неверные слова на верные.\n",
    "    На выходе получается список той же длины но со словами без опечаток'''\n",
    "\n",
    "    pel = YandexSpeller()\n",
    "\n",
    "    correct_tokens_list = []\n",
    "    for word in tokens_list:\n",
    "        correct_tokens_list.append(pel.spelled(word))\n",
    "\n",
    "    assert len(tokens_list) == len(correct_tokens_list)\n",
    "\n",
    "    return correct_tokens_list\n",
    "\n",
    "\n",
    "# проверка на наличие опечаток в тексте\n",
    "def check_typos(text: str):\n",
    "    '''В данной функции сравнивается список токенов исходного текста\n",
    "    со списком токенов прогнанных через функция проверки опечаток:\n",
    "    в случае совпадения (успеха) возвращается 0, в случае несоответствия возвращается 1,\n",
    "    что означает, что в тексте присутствуют опечатки'''\n",
    "\n",
    "    tokens_list = text_preprocess(text)\n",
    "    correct_list = correct_typos(tokens_list)\n",
    "    for i, j in zip(tokens_list, correct_list):\n",
    "        if j != i:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "# проверка на наличие обсценной лексики\n",
    "def check_obscene(text: str, vocab_of_obscene: list = vocab_obscene):\n",
    "    '''Функция ищет соответствия токенов из текста словам из словаря обсценной лексики.'''\n",
    "\n",
    "    tokens_list = text_preprocess(text)\n",
    "    for token in tokens_list:\n",
    "        if token in vocab_of_obscene:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# проверка на наличие слов, обозначающих запрещенный товар к продаже\n",
    "def check_forbidden_goods(text: str, vocab_of_forbidden_goods: list = vocab_forbidden):\n",
    "    '''Функция ищет соответствия токенов текста словам из списка наименований запрещенной продукции'''\n",
    "\n",
    "    tokens_list = text_preprocess(text)\n",
    "    model = Mystem()\n",
    "    lemmas = []\n",
    "    for word in tokens_list:\n",
    "        a = model.lemmatize(word)\n",
    "        if a[0] in vocab_of_forbidden_goods:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Проверка пропусков в текстовых полях для полной формы\n",
    "def check_gaps(names_values_list: list):\n",
    "    lst = [None, '', ' ', '\\n', '\\t']\n",
    "    for text_name in names_values_list:\n",
    "        if text_name == (i for i in lst):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "# Проверка пропусков в текстовых полях для упрощенной формы\n",
    "def checks_gaps_simple(text: str):\n",
    "    lst = [None, '', ' ', '\\n', '\\t']\n",
    "    if text_name == (i for i in lst):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Проверка на наличие в форме не менее 4-х характеристик\n",
    "def check_lenght(lenght_chars: int):\n",
    "    '''Обязательных полей на данный момент 3,\n",
    "        в заявке необходимо указать дополнительно не менее 4 характеристик товара.\n",
    "        При изменении значений параметров необходимо изменить в функции их сумму.'''\n",
    "\n",
    "    if lenght_chars >= 7:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# Проверка на наличие в поле \"Описание\" количества символов не менее 50\n",
    "def check_lenght_simple(text: str):\n",
    "    if len(text) <= 50:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Функция чтения из датафрема поля с характеристиками товара\n",
    "def data_extract(data):\n",
    "    char_dict = {}\n",
    "    list_of_chars = json.loads(data['Исходные характеристики'].values[0])\n",
    "    for char in range(len(list_of_chars)):\n",
    "        char_dict[char] = list_of_chars[char]\n",
    "    return char_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция-агрегатор тестов\n",
    "def checks(text: str, form_type: str,\n",
    "           vocab_of_obscene: list,\n",
    "           vocab_of_forbidden_goods: list,\n",
    "           lenght_chars: int = None,\n",
    "           names_values_list: list = None):\n",
    "    '''В данной функции проводятся тесты для двух видов заявок: Полная и Упрощення.\n",
    "        Разница в обработке входных данных.\n",
    "        Проводимые тесты:\n",
    "                - на наличие опечаток;\n",
    "                - на наличие обсценной лексики;\n",
    "                - на наличие наименований запрещенных товаров;\n",
    "                - на пропуски в текстовых полях;\n",
    "                - на количество указанных характеристик товара'''\n",
    "\n",
    "    test_typos = check_typos(text)\n",
    "    test_obscene = check_obscene(text, vocab_of_obscene)\n",
    "    test_forbidden = check_forbidden_goods(text, vocab_of_forbidden_goods)\n",
    "\n",
    "    if form_type == ['Полная']:\n",
    "        test_ln = check_lenght(lenght_chars)\n",
    "        test_gaps = check_gaps(names_values_list)\n",
    "\n",
    "    else:\n",
    "        test_ln = check_lenght_simple(text)\n",
    "        test_gaps = check_gaps(text)\n",
    "\n",
    "    sum_list = {'number_of_chars': test_ln,\n",
    "                'gaps_in_fields': test_gaps,\n",
    "                'typos_in_text_fields': test_typos,\n",
    "                'obscene_in_text_fields': test_obscene,\n",
    "                'forbidden_goods': test_forbidden}\n",
    "\n",
    "    return sum_list\n",
    "\n",
    "\n",
    "# Аггрегирующая функция для всех тестов\n",
    "def form_checking_tests(data, vocab_of_obscene: list, vocab_of_forbidden_goods: list):\n",
    "    form_type = list(data['Форма заявки'])\n",
    "\n",
    "    if form_type == ['Полная']:\n",
    "        chars = data_extract(data)\n",
    "        text_all_chars = []\n",
    "        names_values_list = []\n",
    "        for v in chars.values():\n",
    "            text = str(v['name']) + ' ' + str(v['value'])\n",
    "            names_values_list.append(v['name'])\n",
    "            names_values_list.append(v['value'])\n",
    "            text_all_chars.append(text)\n",
    "\n",
    "        text_all_chars_txt = ','.join(text_all_chars)\n",
    "        lenght_chars = len(chars)\n",
    "        results = checks(text_all_chars_txt, form_type, vocab_obscene, vocab_forbidden, lenght_chars, names_values_list)\n",
    "\n",
    "    elif form_type == ['Упрощенная']:\n",
    "        text = str(data['Описание'])\n",
    "        results = checks(text, form_type, vocab_obscene, vocab_forbidden)\n",
    "\n",
    "    else:\n",
    "        raise KeyError\n",
    "\n",
    "    # проводим тесты для наименования товара\n",
    "    good_name = str(data['Наименование'])\n",
    "    test_name_plur = check_plural(good_name)\n",
    "    test_text_isenglish = isEnglish(good_name)\n",
    "    test_name_typos = check_typos(good_name)\n",
    "    test_name_obscene = check_obscene(good_name, vocab_of_obscene)\n",
    "    test_name_forbidden = check_forbidden_goods(good_name, vocab_of_forbidden_goods)\n",
    "\n",
    "    tests_list = [test_name_plur, test_text_isenglish, test_name_typos, test_name_obscene, test_name_forbidden]\n",
    "    tests_names = ['plural_good_name', 'english_good_name', 'typos_in_good_name',\n",
    "                   'obscene_in_good_name', 'forbidden_good']\n",
    "\n",
    "    for name, res in zip(tests_names, tests_list):\n",
    "        results[name] = res\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def text_results(results: dict, provider_comments: dict, moderator_comments: dict):\n",
    "    '''Функция ищет соответствия итогов результатов между словарями\n",
    "    для конечного вывода для поставщика и модератора'''\n",
    "\n",
    "    result_comment_for_provider = []\n",
    "    result_comment_for_moderator = []\n",
    "\n",
    "    for k, v in results.items():\n",
    "        if v == 1:\n",
    "            result_comment_for_provider.append(provider_comments[k])\n",
    "            result_comment_for_moderator.append(moderator_comments[k])\n",
    "\n",
    "    if len(result_comment_for_provider) != 0:\n",
    "        result_comment_for_provider.append(provider_comments['comment_neg'])\n",
    "    else:\n",
    "        result_comment_for_provider.append(provider_comments['comment_pos'])\n",
    "    return (\n",
    "        f'Результат проверки заявки для поставщика: {result_comment_for_provider}. Результат проверки заявки для модератора: {result_comment_for_moderator}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import timm\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast, FeatureExtractionPipeline\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict, namedtuple\n",
    "import torchtext\n",
    "import random\n",
    "random.seed(666)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import timm\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "from clip.simple_tokenizer import SimpleTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "image_path = \"/storage1/ryazantsev/AminoEmbeddings/TenderHack/sku_images/sku_images/\"\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Normalize, Resize,\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def get_transforms_val():\n",
    "    return Compose([\n",
    "            Resize(224, 224),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "class nfnet_rubert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nfnet_rubert, self).__init__()\n",
    "        self.bert  = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "        self.enet = timm.create_model('nfnet_l0', pretrained= False)\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size+1000, 512)\n",
    "        self.output = nn.Linear(512, 1)\n",
    " \n",
    "    def forward(self, x,input_ids_title, attention_mask_title):\n",
    "        x = self.enet(x)\n",
    "        #print(x.shape)\n",
    "        text_title = self.bert(input_ids=input_ids_title, attention_mask=attention_mask_title)[0][:,0,:]\n",
    "        #text_title = mean_pooling(text_title, attention_mask_title)\n",
    "        x = torch.cat([x, text_title], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    def load_weights(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "model_matching = nfnet_rubert()\n",
    "model_matching.eval();\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "model_matching.load_weights(\"/storage1/ryazantsev/AminoEmbeddings/TenderHack/models/matching_img_textF1_0.804.pt\")\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "preprocessing_matching = get_transforms_val()\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "threshold_matching = 0.571428\n",
    "tokenizer_matching = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "def inference(args):\n",
    "    image = str(args[0])\n",
    "    image = cv2.imread(image_path + image.split('.')[0] + \".jpg\")\n",
    "    image = preprocessing(image=image)['image']\n",
    "    image = image.reshape(1,3,224,224)\n",
    "    \n",
    "    title = tokenizer(list([args[1]]), padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    #print(title)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model_matching(image, title['input_ids'], title['attention_mask'])\n",
    "        prob_out = F.sigmoid(output).detach().cpu().numpy()    \n",
    "    \n",
    "    return prob_out[0][0] > 0.571428\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import timm\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(666)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import timm\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "class nfnet_watermark(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nfnet_watermark, self).__init__()\n",
    "        self.enet = timm.create_model('nfnet_l0', pretrained= False)\n",
    "        self.n_features = self.enet.head.fc.in_features\n",
    "        self.enet.head.fc= nn.Linear(self.n_features, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enet(x)\n",
    "        return x\n",
    "    \n",
    "    def load_weights(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "def get_transforms_val():\n",
    "    return Compose([\n",
    "            Resize(224, 224),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, Normalize, Resize,\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# In[92]:\n",
    "\n",
    "\n",
    "image_path = \"/storage1/ryazantsev/AminoEmbeddings/TenderHack/sku_images/sku_images/\"\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "\n",
    "model = nfnet_watermark()\n",
    "model.eval();\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "\n",
    "model.load_weights('/storage1/ryazantsev/AminoEmbeddings/TenderHack/models/realese_watermark.pt')\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "preprocessing = get_transforms_val()\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "threshold = 0.40816\n",
    "\n",
    "\n",
    "# In[122]:\n",
    "\n",
    "\n",
    "def inference_watermark(image):\n",
    "    image = cv2.imread(image_path + str(image).split('.')[0] + \".jpg\")\n",
    "    image = np.array(image)\n",
    "    image = preprocessing(image=image)['image']\n",
    "    \n",
    "    image = image.reshape(1,3,224,224)\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prob_out = F.sigmoid(output).detach().cpu().numpy()\n",
    "    return prob_out[0][0] > 0.40816, prob_out[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T07:03:42.741961Z",
     "iopub.status.busy": "2021-06-13T07:03:42.741572Z",
     "iopub.status.idle": "2021-06-13T07:03:42.765139Z",
     "shell.execute_reply": "2021-06-13T07:03:42.764148Z",
     "shell.execute_reply.started": "2021-06-13T07:03:42.741894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CountryInManufacturer(BaseStep):\n",
    "    def __pre_init__(self,):\n",
    "        countries = pd.read_csv('country.csv', usecols=['name'], sep='\\t')\n",
    "        countries = countries.name.to_list()\n",
    "        countries.append('РФ')\n",
    "        countries.append('Российская Федерация')\n",
    "        countries.append('Голландия')\n",
    "        self.countries = [country.upper() for country in countries]\n",
    "        \n",
    "    def _eval_function(self, init_char):\n",
    "        for item in json.loads(init_char):\n",
    "            if item['name'] == \"Производитель\":\n",
    "                return (item['value'].upper() in self.countries) if item['value'] else False\n",
    "        return False\n",
    "\n",
    "class ChechRepeatCharacteristics(BaseStep):\n",
    "    def _eval_function(self, chars):\n",
    "        def check_repeat_prod_type(chars):\n",
    "            i = 0\n",
    "            for ch in chars:\n",
    "                if ch:\n",
    "                    if ('Вид товаров' in ch):\n",
    "                        i += 1\n",
    "            return i > 1\n",
    "        char_is_repeat = lambda x: len(x) > len(set(x))\n",
    "        return any([char_is_repeat(chars), check_repeat_prod_type(chars)])\n",
    "    \n",
    "class CheckForOfferNum(BaseStep):\n",
    "    def _eval_function(self, args):\n",
    "        name = args[0]\n",
    "        offer_number = args[1]\n",
    "        if offer_number and name:\n",
    "            return bool(re.findall(f\"^{re.escape(str(offer_number))}\", str(name)))\n",
    "        return False\n",
    "\n",
    "    \n",
    "class CheckUnitsInName(BaseStep):\n",
    "    def __pre_init__(self,):\n",
    "        top_units = char_units_df.iloc[:50, 0].to_list()\n",
    "        top_units.remove('')\n",
    "        self.top_units = top_units\n",
    "    \n",
    "    def _eval_function(self, name):\n",
    "        if name:\n",
    "            return any([any(re.findall(r'[\\s|\\d]+' + re.escape(f\"{str(sub)}\") + r'[\\s|\\d|\\.|$|/]+', str(name))) for sub in self.top_units])\n",
    "    \n",
    "    \n",
    "class CheckUnitsInCharValues(BaseStep):\n",
    "    def __pre_init__(self,):\n",
    "        top_units = char_units_df.iloc[:50, 0].to_list()\n",
    "        top_units.remove('')\n",
    "        self.top_units = top_units\n",
    "    \n",
    "    def _eval_function(self, char_values):\n",
    "        result = []\n",
    "        if char_values:\n",
    "            for val in char_values:\n",
    "                result.append(any([any(re.findall(r'[\\s|\\d]+' + re.escape(f\"{str(sub)}\") + r'[\\s|\\d|\\.|$|/]+', str(val))) for sub in self.top_units]))\n",
    "        return any(result)\n",
    "    \n",
    "    \n",
    "class CheckUnitsInCharNames(BaseStep):\n",
    "    def __pre_init__(self,):\n",
    "        top_units = char_units_df.iloc[:50, 0].to_list()\n",
    "        top_units.remove('')\n",
    "        top_units.remove('штук')\n",
    "        top_units.remove('листов')\n",
    "        self.top_units = top_units\n",
    "    \n",
    "    def _eval_function(self, char_names):\n",
    "        result = []\n",
    "        if char_names:\n",
    "            for name in char_names:\n",
    "                result.append(any([any(re.findall(r'[\\s|\\d]+' + re.escape(f\"{str(sub)}\") + r'[\\s|\\d|\\.|$|/]+', str(name))) for sub in self.top_units]))\n",
    "        return any(result)\n",
    "    \n",
    "\n",
    "class CheckNamePicMatch(BaseStep):\n",
    "    def _eval_function(self, args):\n",
    "        return inference(args)\n",
    "    \n",
    "    \n",
    "class CheckWatermarks(BaseStep):\n",
    "    def _eval_function(self, image):\n",
    "        return inference_watermark(image)\n",
    "    \n",
    "\n",
    "class CheckTypos(BaseStep):\n",
    "    def _eval_function(self, name):\n",
    "        return check_typos(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T07:07:44.105002Z",
     "iopub.status.busy": "2021-06-13T07:07:44.104622Z",
     "iopub.status.idle": "2021-06-13T07:07:44.119400Z",
     "shell.execute_reply": "2021-06-13T07:07:44.118392Z",
     "shell.execute_reply.started": "2021-06-13T07:07:44.104936Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp = Pipeline(steps=[CountryInManufacturer(name='CountryInManufacturer', columns=['init_char'], pos_status='в карточке заявки в поле \"Производитель\" - должна быть отражена компания производитель, а не страна, в которой был произведен товар', neg_status='OK'),\n",
    "                     ChechRepeatCharacteristics(name='ChechRepeatCharacteristics', columns=['char_names'], pos_status='в списке наименований характеристик не должно быть дублирования характеристик и характеристик не свойственных СТЕ', neg_status='OK'),\n",
    "                     CheckForOfferNum(name='CheckForOfferNum', columns=['name', 'offer_number'], pos_status='наименование заявки не должно начинаться с артикула', neg_status='OK'),\n",
    "                     CheckUnitsInCharValues(name='CheckUnitsInCharValues', columns=['char_values'], pos_status='в блоке \"Характеристики СТЕ\" единицы измерения расположены в поле \"Значение\"', neg_status='OK'),\n",
    "                     CheckUnitsInName(name='CheckUnitsInName', columns=['name'], pos_status='единицы измерения расположены в поле \"Наименование\"', neg_status='OK'),\n",
    "                     CheckUnitsInCharNames(name='CheckUnitsInCharNames', columns=['char_names'], pos_status='в блоке \"Характеристики СТЕ\" единицы измерения расположены в поле \"Название характеристики\"', neg_status='OK'),\n",
    "                     #CheckNamePicMatch(name='CheckNamePicMatch', columns=['name', 'image'], pos_status='описание товара (наименование, характеристики) не совпадает с представленным товаром на изображении', neg_status='OK'),\n",
    "                     #CheckWatermarks(name='CheckNamePicMatch', columns=['image'], pos_status='изображение не должно содержать логотипа, оттисков печатей, водяных знаков, QR-кода, ссылок на веб-ресурсы, а также иных документов', neg_status='OK'),\n",
    "                     CheckTypos(name='CheckTypos', columns=['name'], pos_status='ошибка в наименовании СТЕ', neg_status='OK'),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T07:03:56.996626Z",
     "iopub.status.busy": "2021-06-13T07:03:56.996493Z",
     "iopub.status.idle": "2021-06-13T07:03:57.028913Z",
     "shell.execute_reply": "2021-06-13T07:03:57.028478Z",
     "shell.execute_reply.started": "2021-06-13T07:03:56.996611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7694fb6223d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-d998ba3e9a3c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, df, return_status)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mresults_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ac9d49479583>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, df, return_status)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[0;32m--> 296\u001b[0;31m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 )\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-d6b7b8b01cd4>\u001b[0m in \u001b[0;36m_eval_function\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCheckNamePicMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseStep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_eval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-41002be5fe14>\u001b[0m in \u001b[0;36minference\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                     )\n\u001b[1;32m     86\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/lib/python3.6/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fill_value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fill_value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cols\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rows\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "pp.evaluate(df.iloc[5000:5001], return_status=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T07:08:15.055086Z",
     "iopub.status.busy": "2021-06-13T07:08:15.054859Z",
     "iopub.status.idle": "2021-06-13T07:14:23.847686Z",
     "shell.execute_reply": "2021-06-13T07:14:23.847265Z",
     "shell.execute_reply.started": "2021-06-13T07:08:15.055071Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a1, a2, a3 = pp.evaluate(df, return_status=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
